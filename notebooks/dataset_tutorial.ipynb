{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e3a47c-4bb8-4046-a094-4f9a6c57b8be",
   "metadata": {},
   "source": [
    "In this tutorial you will learn:\n",
    " - How the `Identifier` class is used to manage biomteric datasets\n",
    " - How to work with the `DataLoader` and `Dataset` classes \n",
    " - How to load data from a directory of images and assign them identifiers\n",
    " - How to preprocess the data for training a DeepFinger model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d42588-1f63-4ba3-bccc-cfdddd28df51",
   "metadata": {},
   "source": [
    "## Identifier, DataLoader and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba76984",
   "metadata": {},
   "source": [
    "In our codebase, an `Identifier` has a `subject` (to differentiate distinct fingers) and an `impression` (to differentiate the impressions / samples taken from one finger).\n",
    "\n",
    "Although one person can have multiple fingers, we will treat each finger as a separate subject (class) for training the DeepFinger model. This has the advantage, that the model can be trained with ten times as many classes as if we would treat each person as a class. Additionally, some datasets may not provide all ten fingers for each person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6c439-62de-4c60-ab8b-9cc6381f9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.dataset import Identifier\n",
    "\n",
    "# Create an identifier for the first subject (first distinct finger, may belong to the same person as subject 1, 2, 3, ... 9) and second impression\n",
    "# Note that we start counting from 0. This has practical reasons during training.\n",
    "myid = Identifier(subject=0, impression=1)\n",
    "print(myid)\n",
    "\n",
    "assert myid == Identifier(0, 1) # With positional arguments\n",
    "assert myid != Identifier(0, 3) # Equals only if both subject and impression match\n",
    "assert myid != Identifier(1, 1) # Equals only if both subject and impression match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e1009-4d6c-4185-93a6-abe2af10cdcb",
   "metadata": {},
   "source": [
    "To manage multiple identifiers, we use an `IdentifierSet`. This data structure has many useful functions for filtering the contained `Identifier`s and also ensures their uniqueness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8296863-4a02-44ae-af50-c5e026d9747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.dataset import IdentifierSet\n",
    "\n",
    "id_set = IdentifierSet(\n",
    "    [\n",
    "        Identifier(0, 1),\n",
    "        Identifier(3, 1),\n",
    "        Identifier(5, 2),\n",
    "        Identifier(3, 0),\n",
    "        Identifier(1, 2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Total number of identifiers\n",
    "print(len(id_set))\n",
    "\n",
    "# Count how many different subjects\n",
    "print(id_set.num_subjects)\n",
    "\n",
    "# We can access the identifiers by index. Note that they are always sorted by (subject, impression)\n",
    "print(id_set[0])\n",
    "print(id_set[1])\n",
    "print(id_set[2])\n",
    "print(id_set[3])\n",
    "print(id_set[4])\n",
    "\n",
    "# We can filter the Indentifier set according to these indices:\n",
    "id_set_1to3 = id_set.filter_by_index([1, 2, 3])\n",
    "\n",
    "# We can also check if one set is a subset of another (useful to check if some dataset is complete)\n",
    "print(f\"id_set is superset of id_set_1to3: {id_set >= id_set_1to3}\")\n",
    "print(f\"id_set is subset of id_set_1to3: {id_set <= id_set_1to3}\")\n",
    "\n",
    "# identifiers can also be used as keys of a dictionary\n",
    "letter_dict = {id:letter for id, letter in zip(id_set, \"abcde\")}\n",
    "\n",
    "print(\"Letter for myid: \" + letter_dict[myid])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b474fb0-76ea-4f73-955d-8c28dfdab175",
   "metadata": {},
   "source": [
    "Now that we are familiar with how `Identifiers` work, understanding `DataLoader`s and `Dataset`s is not difficult.\n",
    "\n",
    "To implement your own `DataLoader` class you just need to derive from the abstract `DataLoader` class and then implement the `get` classmethod. This method takes a single identifier as input and returns the loaded value. There is no constraint on what can be loaded and in the `flx` codebase there are loaders for fingerprint, minutia maps, embedding vectors and other types of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229a30e-4953-40ab-a3ce-91ea8bab36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from flx.data.dataset import DataLoader\n",
    "\n",
    "class StringLoader(DataLoader):\n",
    "    def __init__(self, value_dict: dict[Identifier, str]):\n",
    "        self.value_dict = value_dict\n",
    "\n",
    "    def get(self, identifier: Identifier) -> str:\n",
    "        return self.value_dict[identifier]\n",
    "\n",
    "string_loader = StringLoader(letter_dict)\n",
    "print(string_loader.get(myid))\n",
    "\n",
    "class RandomNumberLoader(DataLoader):\n",
    "    def get(self, identifier: Identifier) -> float:\n",
    "        return random.random()\n",
    "\n",
    "random_number_loader = RandomNumberLoader()\n",
    "print(random_number_loader.get(myid))\n",
    "print(random_number_loader.get(myid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310751c-cc9c-4276-bab3-68ff5407ea1f",
   "metadata": {},
   "source": [
    "A `Dataset` is just a combination of an `IdentifierSet` and a `DataLoader`. Basically we say what is in the dataset through the `IdentifierSet` and how to access the actual values through the `DataLoader`. An advantage of this separation is, that we can easily create subsets and combine datasets, regardless of what kind of values we have in our `DataLoader`. We can even \"zip\" multiple datasets together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93dc3cf-f5aa-45bd-99dd-bebb60f04071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.dataset import Dataset\n",
    "\n",
    "string_dataset = Dataset(string_loader, id_set)\n",
    "\n",
    "# Datasets also have a get method\n",
    "assert string_dataset.get(myid) == string_loader.get(myid)\n",
    "\n",
    "# But they allow indexed access and iteration as well!\n",
    "for i, s in enumerate(string_dataset):\n",
    "    print(f\"{i}: {s}\")\n",
    "\n",
    "# We get the IdentifierSet via the \"ids\" property\n",
    "print(string_dataset.ids)\n",
    "\n",
    "\n",
    "# Now we see how we can zip datasets\n",
    "random_number_dataset = Dataset(random_number_loader, id_set)\n",
    "for tpl in Dataset.zip(string_dataset, random_number_dataset):\n",
    "    print(tpl)\n",
    "\n",
    "# And how to concatenate them\n",
    "some_other_ids = IdentifierSet([Identifier(i, 0) for i in range(10)])\n",
    "random_number_dataset2 = Dataset(random_number_loader, some_other_ids)\n",
    "\n",
    "# Here the subjects are shared, which means that subject 1 in dataset 1 is the same as subject 1 in dataset 2.\n",
    "# The Identifiers in the new dataset will therefore have the same subject but maybe a different impression\n",
    "# (as the same impression can in multiple datasets)\n",
    "shared_subjects = Dataset.concatenate(random_number_dataset, random_number_dataset2, share_subjects = True)\n",
    "for id in shared_subjects.ids:\n",
    "    print(id)\n",
    "\n",
    "# Here the subjects are not shared, which means that subject 1 in dataset 1 is a different subjects as subject 1 in dataset 2.\n",
    "# The Identifiers in the new dataset will therefore have the same impression but maybe a different subject\n",
    "# (as the same subject can appear in multiple datasets)\n",
    "separate_subjects = Dataset.concatenate(random_number_dataset, random_number_dataset2, share_subjects = False)\n",
    "for id in separate_subjects.ids:\n",
    "    print(id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041560c-d4d4-45c3-b6ea-2b6629ca6c57",
   "metadata": {},
   "source": [
    "## Loading an image dataset from disk\n",
    "\n",
    "For loading image datasets from disk there exists the `ImageLoader` class. It does most of the work of loading and indexing the image files inside a root dir (and its subdirectories).\n",
    "\n",
    "Take a look at the following folder structure, containing fingerprint images and fingerprint iso templates (.ist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87424353-ef31-4747-ac5f-15a678de5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "# NOTE: If this does not work, enter the absolute path to the notebooks/example-dataset directory here! \n",
    "example_dataset_path = os.path.abspath(\"example-dataset\")\n",
    "list_files(example_dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dae22c-fd52-4ce6-bb52-71a3ee4ea8e8",
   "metadata": {},
   "source": [
    "As one can see, the folder structure is rather complicated, however, the subject and impression can be read from the filename directly. We can use this to index and access the whole directory with just a few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28e564-1687-4c40-b077-d2c8f438e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.data.image_loader import ImageLoader\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as VTF\n",
    "import PIL\n",
    "from IPython.display import display\n",
    "\n",
    "# Derive from ImageLoader and implement the following three staticmethods:\n",
    "class ExampleImageLoader(ImageLoader):\n",
    "    # We do not need to override the __init__ method. In case you need to do it, call\n",
    "    # super().__init__(<my_root_dir>)\n",
    "    # with the root dir of the image dataset inside your __init__ method.\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extension() -> str:\n",
    "        return \".png\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _file_to_id_fun(subdir: str, filename: str) -> Identifier:\n",
    "        # We can ignore the subdir\n",
    "        # But the filename has the pattern: <subject>_<impression>.png\n",
    "        subject_id, impression_id = filename.split(\"_\")\n",
    "        return Identifier(int(subject_id), int(impression_id))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_image(filepath: str) -> torch.Tensor:\n",
    "        img = PIL.Image.open(filepath)\n",
    "        img = PIL.ImageOps.grayscale(img)\n",
    "        # TODO: Resize / crop your image to the input size of DeepPrint (which is 299 x 299)#\n",
    "        # Take a look at flx.data.image_loader to see how this can be done.\n",
    "        \n",
    "        # To be compatible with DeepPrint, we convert it to pytorch.Tensor\n",
    "        # The image is now in format 1 x height x width and pixel values are scaled from [0. 255] to [0. 1]\n",
    "        return VTF.to_tensor(img)\n",
    "        \n",
    "image_loader = ExampleImageLoader(example_dataset_path)\n",
    "expected_ids = IdentifierSet([Identifier(i, j) for i in range(1, 11) for j in range(1, 11)])\n",
    "assert image_loader.ids == expected_ids # Not all DataLoaders have an identifier set, but ImageLoaders always have\n",
    "\n",
    "image_dataset = Dataset(image_loader, expected_ids)\n",
    "\n",
    "img = image_dataset[0]\n",
    "print(img.shape)\n",
    "display(VTF.to_pil_image(img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8428691-5ce0-40f4-9978-731536cf07f4",
   "metadata": {},
   "source": [
    "## Preprocessing images\n",
    "\n",
    "You can preprocess and augment your image using the `TransformedImageLoader` class. It accepts pose transformation (shift, rotation) which are applied first, followed by any number of image processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d913e3-37a0-4bb5-b1ab-c9a0c08a76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flx.image_processing.augmentation import RandomPoseTransform\n",
    "from flx.image_processing.binarization import LazilyAllocatedBinarizer\n",
    "from flx.data.transformed_image_loader import TransformedImageLoader\n",
    "\n",
    "transformed_images = TransformedImageLoader(\n",
    "    image_loader, # Just the regular image loader\n",
    "    poses = RandomPoseTransform(), # Augment the pose; Use this during training\n",
    "    transforms = [LazilyAllocatedBinarizer(ridge_width = 5.0)] # Apply Gabor Wavelets and binarize\n",
    ")\n",
    "\n",
    "image_dataset = Dataset(transformed_images, expected_ids)\n",
    "\n",
    "img = image_dataset[0]\n",
    "print(img.shape)\n",
    "display(VTF.to_pil_image(img))\n",
    "\n",
    "# Notice, that the pose transform adds padding. You may have to add a transform that crops / resizes it again to the correct size...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c21b2",
   "metadata": {},
   "source": [
    "## Testing SOCOFing and Crossmatch Datasets\n",
    "\n",
    "1. **SOCOFing**: 6000 fingerprints from 600 subjects (10 fingers each) - Primary training dataset\n",
    "2. **Crossmatch**: Multiple impressions of the same finger - Verification training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad1204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IdentifierSet with 6000 subjects and a total of 6000 samples.\n",
      "   Total samples: 6000\n",
      "   Number of subjects: 6000\n",
      "\n",
      "First 5 identifiers:\n",
      "   1. Subject: 0, Impression: 0\n",
      "   2. Subject: 1, Impression: 0\n",
      "   3. Subject: 2, Impression: 0\n",
      "   4. Subject: 3, Impression: 0\n",
      "   5. Subject: 4, Impression: 0\n",
      "Created IdentifierSet with 1 subjects and a total of 1 samples.\n",
      "\n",
      "Impressions for subject 0: 1\n",
      "   Subject 0, Impression 0\n",
      "   Subject 1, Impression 0\n",
      "   Subject 2, Impression 0\n",
      "   Subject 3, Impression 0\n",
      "   Subject 4, Impression 0\n",
      "   Subject 5, Impression 0\n",
      "   Subject 6, Impression 0\n",
      "   Subject 7, Impression 0\n",
      "   Subject 8, Impression 0\n",
      "   Subject 9, Impression 0\n",
      "\n",
      "  Shape: torch.Size([1, 299, 299])\n",
      "Dtype: torch.float32\n",
      "Value range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from flx.data.image_loader import SOCOFingLoader, CrossmatchLoader\n",
    "\n",
    "SOCO_DIR = os.path.abspath(\"dataset/soco\")\n",
    "CROSSMATCH_DIR = os.path.abspath(\"dataset/crossmatch\")\n",
    "\n",
    "soco_loader = SOCOFingLoader(SOCO_DIR)\n",
    "print(f\"   Total samples: {len(soco_loader.ids)}\")\n",
    "print(f\"   Number of subjects: {soco_loader.ids.num_subjects}\")\n",
    "print(\"\\nFirst 5 identifiers:\")\n",
    "\n",
    "for i, identifier in enumerate(list(soco_loader.ids)[:5]):\n",
    "    print(f\"   {i+1}. Subject: {identifier.subject}, Impression: {identifier.impression}\")\n",
    "\n",
    "subject_0_impressions = soco_loader.ids.filter_by_subject([0])\n",
    "print(f\"\\nImpressions for subject 0: {len(subject_0_impressions)}\")\n",
    "\n",
    "# Verify person 1's 10 fingers are subjects 0-9\n",
    "person_1_subjects = [id for id in list(soco_loader.ids)[:10]]\n",
    "for id in person_1_subjects:\n",
    "    print(f\"   Subject {id.subject}, Impression {id.impression}\")\n",
    "\n",
    "# Test loading an actual image\n",
    "test_id = list(soco_loader.ids)[0]\n",
    "test_image = soco_loader.get(test_id)\n",
    "print(f\"\\n  Shape: {test_image.shape}\")\n",
    "print(f\"Dtype: {test_image.dtype}\")\n",
    "print(f\"Value range: [{test_image.min():.3f}, {test_image.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49cb6562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created IdentifierSet with 77 subjects and a total of 616 samples.\n",
      "   Total samples: 616\n",
      "   Number of subjects: 77\n",
      "\n",
      "First 10 identifiers:\n",
      "   1. Subject: 120, Impression: 0\n",
      "   2. Subject: 120, Impression: 1\n",
      "   3. Subject: 120, Impression: 2\n",
      "   4. Subject: 120, Impression: 3\n",
      "   5. Subject: 120, Impression: 4\n",
      "   6. Subject: 120, Impression: 5\n",
      "   7. Subject: 120, Impression: 6\n",
      "   8. Subject: 120, Impression: 7\n",
      "   9. Subject: 121, Impression: 0\n",
      "   10. Subject: 121, Impression: 1\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "Created IdentifierSet with 1 subjects and a total of 8 samples.\n",
      "   8 impressions: 77 subjects\n",
      "\n",
      "Test image shape: torch.Size([1, 299, 299])\n",
      "   Image dtype: torch.float32\n",
      "   Image value range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Load Crossmatch dataset\n",
    "crossmatch_loader = CrossmatchLoader(CROSSMATCH_DIR)\n",
    "print(f\"   Total samples: {len(crossmatch_loader.ids)}\")\n",
    "print(f\"   Number of subjects: {crossmatch_loader.ids.num_subjects}\")\n",
    "\n",
    "# Check some sample identifiers\n",
    "print(\"\\nFirst 10 identifiers:\")\n",
    "for i, identifier in enumerate(list(crossmatch_loader.ids)[:10]):\n",
    "    print(f\"   {i+1}. Subject: {identifier.subject}, Impression: {identifier.impression}\")\n",
    "\n",
    "impression_counts = defaultdict(int)\n",
    "subjects_seen = set()\n",
    "\n",
    "for identifier in crossmatch_loader.ids:\n",
    "    subject_id = identifier.subject\n",
    "    if subject_id not in subjects_seen:\n",
    "        subjects_seen.add(subject_id)\n",
    "        impressions_for_subject = crossmatch_loader.ids.filter_by_subject([subject_id])\n",
    "        impression_counts[len(impressions_for_subject)] += 1\n",
    "\n",
    "for num_impressions, count in sorted(impression_counts.items()):\n",
    "    print(f\"   {num_impressions} impressions: {count} subjects\")\n",
    "\n",
    "# Test loading an actual image\n",
    "test_id = list(crossmatch_loader.ids)[0]\n",
    "test_image = crossmatch_loader.get(test_id)\n",
    "print(f\"\\nTest image shape: {test_image.shape}\")\n",
    "print(f\"   Image dtype: {test_image.dtype}\")\n",
    "print(f\"   Image value range: [{test_image.min():.3f}, {test_image.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d13a3",
   "metadata": {},
   "source": [
    "## üéØ Training Strategy: Using Both Datasets\n",
    "\n",
    "Based on the documentation and the model training tutorial, here's the optimal strategy:\n",
    "\n",
    "### **Dataset 1: SOCOFing (Primary Training)**\n",
    "- **Size**: 6000 fingerprints from 600 subjects (10 fingers each)\n",
    "- **Use**: Primary feature learning and classification training\n",
    "- **Benefits**: \n",
    "  - Large diverse dataset for robust feature representations\n",
    "  - 600 different subjects provides excellent diversity\n",
    "  - Good for learning discriminative fingerprint features\n",
    "\n",
    "### **Dataset 2: Crossmatch (Verification Training)**\n",
    "- **Size**: Multiple impressions of the same finger (up to 8 scans per finger)\n",
    "- **Use**: Verification/matching capability training\n",
    "- **Benefits**:\n",
    "  - Multiple impressions of same finger perfect for learning intra-class similarity\n",
    "  - Ideal for genuine vs impostor discrimination\n",
    "  - Can be used for validation during training\n",
    "\n",
    "### **Recommended Training Approach** (from model_training_tutorial.ipynb):\n",
    "\n",
    "1. **Phase 1**: Train on SOCOFing for feature learning (30 epochs)\n",
    "2. **Phase 2**: Fine-tune on Crossmatch for verification capability (20 epochs)  \n",
    "3. **Phase 3**: Joint training on balanced mix of both (15 epochs)\n",
    "\n",
    "Or use the simpler approach shown in the tutorial:\n",
    "- Train primarily on SOCOFing\n",
    "- Use Crossmatch as validation set to monitor verification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ecc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üèóÔ∏è  Creating Dataset Objects for Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create Dataset objects (combining loader with transforms)\n",
    "from flx.data.dataset import Dataset\n",
    "\n",
    "# SOCOFing dataset - for main training\n",
    "soco_dataset = Dataset(soco_loader)\n",
    "print(f\"\\nüì¶ SOCOFing Dataset:\")\n",
    "print(f\"   Subjects: {soco_dataset.ids.num_subjects}\")\n",
    "print(f\"   Total samples: {len(soco_dataset.ids)}\")\n",
    "\n",
    "# Crossmatch dataset - for verification/validation\n",
    "crossmatch_dataset = Dataset(crossmatch_loader)\n",
    "print(f\"\\nüì¶ Crossmatch Dataset:\")\n",
    "print(f\"   Subjects: {crossmatch_dataset.ids.num_subjects}\")\n",
    "print(f\"   Total samples: {len(crossmatch_dataset.ids)}\")\n",
    "\n",
    "# Create train/val split for SOCOFing (e.g., 90/10 split)\n",
    "import random\n",
    "all_subjects = list(range(soco_dataset.ids.num_subjects))\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(all_subjects)\n",
    "\n",
    "train_split = int(0.9 * len(all_subjects))\n",
    "train_subjects = set(all_subjects[:train_split])\n",
    "val_subjects = set(all_subjects[train_split:])\n",
    "\n",
    "# Filter datasets\n",
    "soco_train_ids = [id for id in soco_dataset.ids if id.subject in train_subjects]\n",
    "soco_val_ids = [id for id in soco_dataset.ids if id.subject in val_subjects]\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è  SOCOFing Train/Val Split:\")\n",
    "print(f\"   Train: {len(train_subjects)} subjects, {len(soco_train_ids)} samples\")\n",
    "print(f\"   Val:   {len(val_subjects)} subjects, {len(soco_val_ids)} samples\")\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets are ready for training!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
